\documentclass[12pt,a4paper]{article} % using article ensures it starts at 1 and does not have odd numberings for section
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % this way umlaute are included from the get go
\usepackage[ngerman]{babel} % german spell check
\usepackage{datetime}

%\usepackage{breqn} % this package is one option for math lines
\usepackage{mathtools} % this package contains math functions and a kind of table generator (align)
\usepackage{mathdots}

\usepackage{hyperref} % these two lines are so that the table of content is clickable
\hypersetup{linktoc=all}

\usepackage{amssymb} % package for Natural Number sign etc

\usepackage[makeroom]{cancel}

%added commands:
\newcommand*\conj[1]{\overline{#1}}
\newcommand*\mean[1]{\bar{#1}}
\newcommand*\tab[1][1cm]{\hspace*{#1}}
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}} % Diagonal slash fraction


\begin{document}
	\tableofcontents % creats a table of contents, ensured already that it is clickable
	\newpage % starts the actual document on a new page so there is no weird colision of text and toc
	\section{Norm und Skalarprodukt}
	
	
	\subsection{Norm}
	Definitheit: $||x|| = 0 \Rightarrow x = 0$
		
	absolute Homogenität: $||\alpha x|| = |\alpha| * ||x||$
	
	Dreiecksungleichung: $||x + y|| \le ||x|| + ||y||$
	
	\subsection{Skalarprodukt}
	
	\[
		\left.
			\begin{array}{l}
				<x + y, z> = <x, z> + <y, z> \\
				<x, y + z> = <x, y> + <x, z> \\
				<\lambda x, y> = \lambda <x, y> \\
				<x, \lambda y> = \lambda <x, y>
			\end{array}
		\right \} \textnormal{Linearität}
	\]
	
	\begin{math}
		<x, y> = <y, x>
	\end{math}
	
	\[
		\left.
			\begin{array}{l}
				<x, x> \ge 0 \\
				<x, x> = 0 \Rightarrow x = 0
			\end{array}
		\right \} positiv Definitheit
	\]
	
	
	\subsubsection{Vom Skalarprodukt induzierte Norm}
	$||x|| = \sqrt{<x, x>}$
	
	\subsubsection{Cauchy-Schwarzche Ungleichung}
	$|<x, y>| \le ||x||*||y||$
		
	\newpage
	
	\section{Symmetrische, positiv definite Matrix}
	
	\begin{equation*}
		\begin{pmatrix}
			a & b \\
			b & c 
		\end{pmatrix}
		\begin{pmatrix}
			a & b & c \\
			b & d & e \\
			c & e & f
		\end{pmatrix}
		\begin{pmatrix}
			a & b & c & d \\
			b & e & f & g \\
			c & f & h & i \\
			d & g & i & j
		\end{pmatrix}
	\end{equation*}
	Symmetrische Matrix
	
	
	insbesonders: Diagonalmatrizen, Einheitsmatrizen
	
	positiv definit: $x^t Ax > 0$ (beliebige Matrix)
	
	alle EW $>$ 0 (symmetrische Matrix)
	
	alle Haupt[TODO: ?] $>$ 0 (symetrische Matrix)
	
	$\begin{pmatrix}
		a & b & c \\
		b & d & e \\
		c & e & f
	\end{pmatrix}$
 	$\Rightarrow$ 3 Hauptminoren[?] = det(a), det
 	$\begin{pmatrix}
	 	a & b \\
	 	b & d 
 	\end{pmatrix}$
 	, det
 	$\begin{pmatrix}
 	a & b & c \\
 	b & d & e \\
 	c & e & f
 	\end{pmatrix}$
	
	\subsection{Cholesky-Zerlegung}
	$A = GG^t$ G unter der Matrix, invertierbar (symmetrische Matrix)
	
	\subsection{[?] diagonaldominant und alle Diagonalelemente größer gleich 0}
	(symmetrische Matrix)
	
	\subsection{Eigenwerte}
	det($\lambda$ En - A) = 0
	
	\subsection{Eigenvektor}
	f(v) = $\lambda$v
		
	\newpage
	
	\section{Matrixnormen}
	
	\subsection{Natürliche Matrixnorm}
	
	$||A||_\infty := \max\limits_{x \ne 0} \frac{||Ax||_\infty}{||x||_\infty} = \max\limits_{||x|| = 1}||Ax||_\infty$
	
	$||A|| = 0 \Rightarrow A = 0 $
	
	$||\lambda A|| = |\lambda|*||A|| $
	
	$||A+B|| \le ||A|| + ||B||$
	
	$||A*B|| \le ||A|| * ||B||$

	
	\subsection{Verträglichkeit}
	$||Ax|| \le ||A|| * ||x||$
	
	\subsection{Zeilensummennorm}
	= natürliche Matrixnorm
	
	$||A||_\infty = \max\limits_{||x||_\infty = 1} ||Ax||_\infty = \max\limits_{i = 1, ..., m} \sum\limits_{j = 1}^{n} |a_{ij}|$
	
	A = 
	$\begin{pmatrix}
		1 & -2 & -3 \\
		2 & 3 & -1
	\end{pmatrix}$
	
	\begin{equation*}
		\begin{split}		
			||A||_\infty & = max\{|1| + |-2| + |-3|, |2| + |3| + |-1|\} \\
			& = max\{6, 6\} = 6
		\end{split}
	\end{equation*}
	
	\subsection{Spaltensummennorm}
	
	$||A||_1 := \max\limits_{x \ne 0} \frac{||Ax||_1}{||x||_1} = \max\limits_{||x||_1 = 1} ||Ax||_1 = \max\limits_{j = 1, ..., n} \sum\limits_{i = 1}^{m}|a_{ij}|$
	
	A = 
	$\begin{pmatrix}
		1 & -2 & -3 \\
		2 & 3 & -1
	\end{pmatrix}$
	
	\begin{equation*}
		\begin{split}		
			||A||_1 & = \max\{|1| + |2|, |-2| + |3|, |-3| + |-1|\}  \\
			& = \max\{3, 5, 4\} = 5
		\end{split}
	\end{equation*}
	
	$||A^t||_1 = ||A||_\infty$
	
	\subsection{Spektralnorm}

	\begin{equation*}
		\begin{split}
			||A||_2 &:= \max\limits_{||x||_2 = 1} ||Ax||_2 \\
			& = \max\limits_{x \ne 0} \frac{||Ax||_2}{||x||_2} \\
			& = \max\limits_{||x||_2 = 1} <Ax, Ax> \\
			& = \max\limits_{||x||_2 = 1} <A^tAx, x> \\
			& = max{\sqrt{|\lambda |}, \lambda * EW von A^tA}
		\end{split}
	\end{equation*}
	
	A = 
	$\begin{pmatrix}
		3 & 2 \\
		-2 & 0 
	\end{pmatrix}$
	, $A^tA$ = 
	$\begin{pmatrix}
		13 & 6 \\
		6 & 4 
	\end{pmatrix}$
	det($\mu E_n - A^tA$) = 0 $\Leftrightarrow \mu_{1, 2}$ = {16, 1}
	
	$||A||_2 = \sqrt{max(\mu_1, \mu_2)} = \sqrt{\mu_1} = \sqrt{16} = 4$
	
	\newpage
	
	\section{Spektralradius, Konditionszahl einer Matrix}
	
	\subsection{Spektralradius \texorpdfstring{$\rho$}{rho}}
	
	$\varphi(A) = max:{1 \le i \le n} |\lambda_i(A)| = spr(A)$ der betragsmäßig größte Eigenwert von A
	
	$||A|| \ge |\lambda|$ (für jede Matrixnorm, die mit einer Vektornorm verträglich ist)
	
	\subsection{Konditionszahl einer Matrix A}
	
	$cond(A) = ||A||*||A^{-1}||$
	
	\subsection{Sonderfall symmetrisch, positiv definite Matrix}
	
	$cond(A) = \frac{ \lambda_{max}}{ \lambda_{min}}$
	
	\newpage
	
	\section{Ähnlichkeitstransformation, Invarianz der Eigenwerte}
	
	y = Ax
	
	\begin{align*}
		\conj{x} = Cx, \conj{y} = Cy && (\text{det C} \ne 0), C \in GL
	\end{align*}
	
	
	$y = Ax \Rightarrow C^{-1} \conj{y} = AC^{-1} \conj{x} \Rightarrow \conj{y} = CAC^{-1} \conj{x} \Rightarrow \conj{y}\conj{A}\conj{x}$
	
	$\conj{A} = CAC^{-1} \Rightarrow \conj{A} \sim A$
	
	$\lambda$ EW, v EV zu A
	
	$\Rightarrow Av = C^{-1}\conj{A}Cv = \lambda v$
	
	$\Rightarrow \conj{A}$ und A haben dieselben Eigenwerte, algebraisch und geometrische Vielfalten stimmen überein (Invarianz der Eigenwerte)
	
	\subsection{Reduktionsmethoden}
	
	A duch Ähnlichkeitstransformationen 
	
	$A = A^{(0)} = T_1^{-1} A^{-1}T_1 = Q ... = T_i^{-1}A^{(i)}T_i = ...$
	
	auf Form bringen, für welche EW und EV leicht zu berechnen sind (z.B. Jordan-Normalform)
	
	\newpage
	
	\section{Gleitkommazahlen, ...}
	
	\subsection{Gleitkommazahl (normalisiert)}
	
	$b \in \mathbb{N}, b \ge 2, x \in \mathbb{R}$
	
	$x = \pm m * b^{\pm e}$
	
	Mantisse: $m = m_1b^{-1} + m_2b^{-2} + ... \in \mathbb{R}$
	
	Exponent: $e = e_{s-1}b^{s-1} + ... + e_0b^0 \in \mathbb{N}$
	
	für $x \ne 0$ eindeutig
	
	\subsection{Gleitkommagitter}
	
	$A = A(b, r, s)$ größte Darstellbare Zahl: $(1 - b^{-r})*b^{b^s-1}$
	
	mit b als Basis, r als Mantissenlänge, s als Exponentenlänge
	
	$(b = 10): 0,314 * 10^1 = 3,14$
	
	$0,123 * 10^6 = 123.000$
	
	Beispiel: konvertiere von Basis 8 zu Basis 10:
	
	$x = (0,5731 * 10^5)_8 \in A(8, 5, 1)$
	
	$x = (5 * 8^{-1} + 7 * 8^{-2} + 3 * 8^{-3} + 1 * 8^{-4}) * 8^5$
	
	$x = 5 * 8^4 + 7 * 8^3 + 3 * 8^2 + 1 * 8^1 = 24.264 * 10^0$
	
	\subsection{Maschienengenauigkeit eps}
	
	$eps = \frac{1}{2}b^{-r + 1}, IEEE: eps = \frac{1}{2} * 2^{-52} \approx 10^{-16}$
	
	\subsection{Rundungsfehler}
	
	$absolut: |x - rd(x)| \le \frac{1}{2}b^{-r}b^e$
	
	$relativ: |\frac{x - rd(x)}{x}| \le \frac{1}{2}b^{-r+1} = eps$
	
	\newpage
	
	\section{Darstellung des Interpolationsfehlers}
	
	\subsection{Fehler I}
	
	f$ \in C^{n+1}$[a, b], $\forall$ x $\in$ [a, b] $\exists \xi_x \in$ ($\overline{x_0, ..., x_n, x}$), wobei das Intervall das kleinst mögliche Intervall, das alle $x_i$ enthällt, s.d.
	
	$f(x) - p(x) = \frac{f^{(n+1)}(\xi x)}{(n+1)!} \prod\limits_{j = 0}^{n}(x - x_j)$
	
	\subsection{Fehler II}
	
	$f \in C^{n + 1}[a, b], \forall x \in [a, b]$ \textbackslash ${x_0, ..., x_n} gilt:$
	
	$f(x) - p(x) = f[x_0, ..., x_n, x] \prod\limits_{j = 0}^{n}(x - x_j)$
	
	mit $f[x_i, ..., x_{i + k}] = y[x_i, ..., x_{i + k}]$
	
	und $f[x_0, ..., x_n, x] = \int\limits_{0}^{1}\int\limits_{0}^{t_1}...\int\limits_{0}^{t_n}f^{n+1}(x_0 + t_1(x_1 - x_0) + ... + t_n(x_n-x_{n - 1} + t(x - x_n))dtdt_n...dt_1$ 
	
	für $x_0 = x_1 = ... = x_n: $
	
	$f[x_0, ..., x_n] = \frac{1}{n!}f^{(n)}(x_0)$
	
	$\frac{f^{(n + 1)}(\xi_x)}{(n + 1)!} \prod\limits_{j = 0}^{n} (x-x_j) = f(x) - p(x) = f[x_0, ..., x_n, x] \prod\limits_{j = 0}^{n}(x - x_j)$
	
	$\Rightarrow f[x_0, ..., x_n, x] = \frac{f^{(n + 1)}(\xi_x)}{(n + 1)!}$
	
	\newpage
	
	\section{Konditionierung einer numerischen Aufgabe, Konditionszahlen \texorpdfstring{$k_{i, j}$}{k i, j}}
	
	\subsection{numerische Aufgabe}
	
	$x_j \in \mathbb{R} \text{ mit }f(x_1, ..., x_m) \Rightarrow y_i = f_i(x_j)$
	
	fehlerhafte Eingangsgrößen $x_i + \Delta y_i$
	
	$|\Delta y_i|$ ist der absolute Fehler, $|\frac{\Delta y_i}{y_i}|$ ist der relative Fehler
	
	\subsection{Konditionszahl (relativ)}
	
	$k_{ij}(x) = \frac{\partial f_i}{\partial x_i}(x) \frac{\Delta x_j}{x_j}$
	
	$\frac{\Delta y_i}{y_i} = \sum\limits_{j = 1}^{m}k_{ij}(x)\frac{\Delta x_j}{x_j}$
	
	$|k_{ij}(x)| >> 1 \Rightarrow$ schlecht konditioniert
	
	$|k_{ij}(x)| << 1 \Rightarrow$ gut konditioniert, ohne Fehlerverstärkung
	
	$|k_{ij}(x)| > 1 \Rightarrow$ Fehlerverstärkung
	
	$|k_{ij}(x)| < 1 \Rightarrow$ Fehlerdämpfung
	
	\newpage
	
	\section{Stabilität eines Algorithmus}
	
	\subsection{stabiler Algorithmus}
	
	akkumulierte Fehler der Rechnung (Rundungsfehler, Auswertungsfehler, etc.) übersteigen den unvermeidbaren Problemfehler der Konditionierung der Aufgabe nicht. Aka Trotz Ungenauigkeiten bei den Eingabe Variablen erhalten wir fast sehr genaue Ergebnisse.
	
	\newpage
	
	\section{Auslöschung}
	
	Verlust von Genauigkeit bei der Subtraktion von Zahlen mit gleichem Vorzeichen
	
	TODO: bei bedarf ein Beispiel
	
	\newpage
	
	\section{Horner-Schema*}
	
	Das sogenannte "Horner-Schema"
	
	\begin{tabular}{l l l}
		$b_n = a_n$ &, k = n - 1, ..., 0 & $b_k = a_k + \xi b_{k + 1} $
	\end{tabular}
	liefert den Funktionswert $p(\xi) = b_0$ des Polynoms
	
	$p(x) = a_0 + x(... + x(a_{n-1} + a_nx)...)$
	
	\subsection{Code}
	
	def horner(Ac, Ax, n, x): 
	
	y = 0.0
	
	for i in reversed range(n):
	
	\tab y = y * (x - Ax[i]) + Ac[i]
 
	return y
	
	Ac: Vektor mit Koeffizienten, ist ein np Array
	
	Ax: Stützstellen, ist ein np Array
	
	n: Anzahl der Stützstellen, ist ein int
	
	x: Auswertungspunkt, ist ein double
	
	Immer Horner-Schema zur Auswertung von Polynomen verwenden.
	
	\subsection{Auswertung}
	TODO: subsection
	
	\newpage
	
	\section{Interpolation und Approximation}
	
	\subsection{Grundproblem}
	
	Darstellung und Auswertung von Funktionen
	
	\subsection{Aufgabenstellung}
	
	f(x) nur auf Diskreter Menge von Argumenten $x_0, ..., x_n$ bekannt und soll rekonstruiert werden
	
	analytisch gegebene Funktion soll auf Reelwerte dargestellt werden, damit jederzeit Werte zu beliebigen x berechnet werden können.
	
	Einfach konstruierte Funktionen in Klassen P:
	
	Polynome: $p(x) = a_0 + a_1x + a_2x^2 + ... + a_nx^n$
	
	rationale Funktion: $r(x) = \frac{a_0 + a_1x + ... + a_nx^n}{b_0 + b_1x + ... + b_mx^m}$
	
	trigonometrische Funktion: $t(x) = \frac{1}{2}a_0 + \sum\limits_{k = 1}^{n}(a_kcos(kx) + b_ksin(kx))$
	
	Exponentialsummen: $e(x) = \sum\limits_{k = 1}^{n}a_kexp(b_kx)$
	
	\subsection{Interpolation}
	
	Zuordnung von $g \in P$ zu f durch Fixieren von Funktionswerten
	
	$g(x_i) = y_i = f(x_i), i = 0, ..., n$
	
	\subsection{Approximation}
	
	$g \in P$ beste Darstellung, z.B. 
	
	$\max\limits_{a \le x \le b}|f(x) - g(x)| minimal$
	
	$(\int\limits_{a}^{b}|f(x) - g(x)|^2dx)^{\frac{1}{2}} minimal$
	
	\newpage
	
	\section{Lagransche Interpolationsaufgabe}
	
	\subsection{Aufgabe}
	
	Finde zu n + 1 verschiedene Stützstellen/Knoten $x_0, ..., x_n \in \mathbb{R}$ und Werten $y_0, ..., y_n \in \mathbb{R}$ ein Polynom p $\in P_n mit p(x_i) = y_i$
	
	\subsection{Eindeutigkeit + Existenz}
	
	Die Lagransche Interpolationsaufgabe ist eindeutig lösbar
	
	TODO: bei bedarf Beweis rein kopieren den Ich nicht verstanden hab
	
	\subsection{Lagransche Basispolynome}
	
	$L_i^{(n)}(x) = \prod\limits_{j = 0, j \ne i}^{n} \frac{x - x_j}{x_i - x_j} \in P_n, i = 0, ..., n$
	
	\subsection{Eigenschaften}
	
	\[
		\textnormal{ortogonal: es gilt $L_i^{(n)}(x_k) = d_{ik} =$} \left\{
			\begin{array}{ll}
				1 & i = k \\
				0 & sonst
			\end{array}
		\right.
	\]
	
	bilden Basis von $P_n$
	
	haben Grad n
	
	\subsection{Lagransche Darstellung}
	
	$p(x) = \sum\limits_{i = 0}^{n}y_iL_i^{(n)}(x) \in P_n$ mit $p(x_j) = y_j$
	
	Nachteil: Bei Hinzunahme von $(x_{n+1}, y_{n+1})$ ändert sich das Basispolynom komplett
	
	
	TODO: Beispiel
	
	\newpage
	
	\section{Newtonsche Basispolynome...}
	
	\subsection{Newton-Polynome}
	
	$N_0(x) = 1, N_i(x) = \prod\limits_{j = 0}^{i - 1}(x - x_j)$ mit $p(x) = \sum\limits_{i = 0}^{n}a_iN_i(x)$
	
	\subsubsection{Auswertung}
	
	$y_0 = p(x_0) = a_0$
	
	$y_1 = p(x_1) = a_0 + a_1 * (x_1 - x_0)$
	
	$\vdots$
	
	$y_n = p(x_n) = a_0 + a_1(x_1 - x_0) + ... + a_n(x_n - x_0) * ... * (x_n - x_{n - 1})$
	
	\subsubsection{Vorteil}
	
	Bei Hinzunahme von $(x_{n + 1}, y_{n + 1})$ muss nur eine neue Rechnung durchgeführt werden, und nicht das gesamte Polynom neu berechnet werden
	
	TODO: Beispiel
	
	\subsection[Newtonsche Darstellung]{Newtonsche Darstellung(stabile Variante)}
	
	$p(x) = \sum\limits_{i = 0}^{n}y[x_0, ..., x_i] N_i(x)$
	
	\subsection{Dividierte Differenzen*}
	
	$y[x_i, ..., x_{k + 1}] = \frac{y[x_{i + 1}, ..., x_{k + 1}] - y[x_i, ..., x_{i + k - 1}]}{x_{i + k} - x_i}$ mit k = 1, ..., j und i = k - j
	
	für beliebige [?] $\sigma:{0, ..., n} \rightarrow {0, ..., n}$ gilt $y[\tilde{x_0}, ..., \tilde{x_n}] = y[x_0, ..., x_n]$
	
	\newpage
	
	\section{Nevillsche Darstellung}
	
	\begin{align*}
		p_{jj}(x) = y_j && j = 0, ..., n && k = 1, ..., j && i = k - j
	\end{align*}
	
	
	$p_{i, i + k}(x) = p_{i, i + k - 1}(x) + (x - x_i)\frac{p_{i + 1, i + k}(x) - p_{i, i + k - 1}(x)}{x_{i + k} - x_i}$
	
	\subsection{Schema}
	
	\begin{align*}
		x && k = 0 && && k = 2 && {}\ldots{} && k = n - 1 && k = n & \\
		x_0 && y_0 && \longrightarrow && p_{0, 1} && {}\ldots{} && p_{0, n - 1} && p_{0, n} & \\
		x_1 && y_1 && \longrightarrow && p_{1, 2} && {}\ldots{} && p_{1, n} & \\
		\vdots && \vdots && \vdots && \iddots & \\
		x_{n - 1} && y_{n - 1} && \longrightarrow && p_{n - 1, n} & \\
		x_n && y_n
	\end{align*}
	
	TODO: add the diagonal arrows
	
	Hinzunahme von $(x_{n + 1}, y_{n + 1})$ ist problemlos
	
	Auswertung von $p_{0, n}(x)$ in $\xi \ne x_i$ ohne vorherige Bestimmung der Koeffizienden der Newton-Darstellung ist einfach und Numerisch stabil möglich
	
	\subsection{Code}
	
	def divDiffs(xi, yi, x):
	
	n = len(xi)
	
	p = n * [0]
	
	for k in range(n):
	
	\tab for i in range(n - k):
	
	\tab \tab if k == 0:

	\tab \tab \tab p[i] = yi[i]
	
	\tab \tab else:
	
	\tab \tab \tab p[i] = ((x - xi[i + k]) * p[i] + (xi[i] - x) * p[i + 1]) / (xi[i] - xi[i + k])
	
	return p[0]	
	
	\newpage
	
	\section{Hermite-Interpolation}
	
	\subsection{Aufgabe}
	
	\begin{align*}
		Gegeben: && x_i && i = 0, ..., m && paarweise verschieden & \\
		&& y_i^{(k)} && i = 0, ..., m && k = 0, ..., \mu_i (\mu_i \ge 0)
	\end{align*}
	
	Gesucht: $p \in P_n$, n = m + $\sum\limits_{i = 0}^{m}\mu_i$ : $p^{(k)}(x_i) = y_i^{(k)}$
	
	$x_i$ sind ($\mu_i$ + 1)-fache Stützstellen
	
	$x_0 = -1$, $x_1 = 1$, $m = 1$, $y_0^{(0)} = 0$, $y_1^{(0)}$, $y_1^{([l?])} = 2$
	
	$\Rightarrow \mu_0 = 0$, $\mu_1 = 1$
	
	$\Rightarrow$ n = 1 + 0 + 1 = 2
	
	$\Rightarrow$ p(x) = $x^2$
	
	\subsection{Existenz + Eindeutig}
	
	analog zur Lagrange-Interpolation
	
	\subsection{Fehler}
	
	$f \in C^{n + 1}[a, b]: \forall x \in [a, b] \exists \xi_x \in (\overline{x_0, ..., x_m, x})$, s.d.
	
	$f(x) - p(x) = f[x_0, ..., x_0, ..., x_m, ..., x_m, x]\prod\limits_{i = 0}^{m}(x - x_i)^{\mu_i + 1} \\
	 = \frac{1}{(n + 1)!}f^{(n + 1)}(\xi_x)\prod\limits_{i = 0}^{m}(x - x_i)^{\mu_i + 1}$
	
	\newpage
	
	\section[Extrapolation]{Extrapolation zum Limes + Fehler}
	
	\subsection{Richardson-Extrapolation}
	
	nicht direkt berechenbare Größe
	
	\begin{align*}
		a(0) = \lim\limits_{k\rightarrow 0}a(k), && k \in \mathbb{R}_+
	\end{align*}
	
	berechne a($k_i$) für gewisse $k_i$, i = 0, ..., n und [?] $p_n(0)$ des Interpolations Polynoms zu $(h_i, a(h_i))$ als Schätzung für a(0)
		
	\begin{tabular}{l l}
		$a(0) := \lim\limits_{x\rightarrow 0^+} \frac{cos(x) - 1}{sin(x)}$ & (= 0)
	\end{tabular}

	$a(x) := \frac{(cos(x) - 1)}{sin(x)}$
	
	Interpolation a(x) an Stützstellen $k_i$ nahe bei 0: 
	
	\begin{tabular}{l l}
		$k_0 = \frac{1}{8}$ & $a(k_0) = -6,258151 * 10^{-2}$ \\
		$k_1 = \frac{1}{16}$ & $a(k_1) = -3,126018 * 10^{-2}$ \\
		$k_2 = \frac{1}{32}$ & $a(k_2) = -1,562627 * 10^{-2}$
	\end{tabular}
	
	\subsection{Lagrange}
	
	$p_2(x) = a(k_0) \frac{(x - \frac{1}{16})(x - \frac{1}{32})}{(\frac{1}{8} - \frac{1}{16})(\frac{1}{8} - \frac{1}{32})} + a(k_1)\frac{(x - \frac{1}{8})(x - \frac{1}{32})}{(\frac{1}{16} - \frac{1}{8})(\frac{1}{16} - \frac{1}{32})} + a(k_2) \frac{(x - \frac{1}{8})(x - \frac{1}{16})}{(\frac{1}{32} - \frac{1}{8})(\frac{1}{32} - \frac{1}{16})}$
	
	$\Rightarrow a(0) \sim p_2(0) = -1,02 * 10^{-5}$
	
	\subsection{Neville}
	
	$p_{i, i + k}(0) = p_{i, i + k - 1}(0) + \frac{p_{i, i + k - 1}(0) - p_{i + 1, i + k}(0)}{\frac{x_{i + k}}{x_i - 1}}$, k = 1, 2
	
	\begin{tabular}{ c | c | c | c | c }
		i & $x_i$ & $p_{i, i}(0) = a(k_i)$ & $p_{i, i + 1}(0)$ & $p_{i, i + 2}(0)$ \\ \hline
		0 & $x_0 = \frac{1}{8}$ & $-6,258151 * 10^{-2}$ & $6,115 * 10^{-5}$ & $-1,02 * 10^{-5}$ \\
		1 & $x_1 = \frac{1}{16}$ & $-3,126018 * 10^{-2}$ & $7,64 * 10^{-6}$ & \\
		2 & $x_2 = \frac{1}{32}$ & $-1,562627 * 10^{-2}$ & & 
	\end{tabular}

	\subsection{Extrapolationsfehler}
	
	a(n) habe die Entwickling:
	
	$a(h) = a_0 + \sum\limits_{j = 1}^{n}a_jh^{jq} + a_{n + 1}(h)h^{(n+1)q}$ \tab mit q $>$ 0,
	 Koeffizienten $a_j$ 
	 
	und $a_{n + 1}(h) = a_{n + 1} + a(1[?????])$
	
	$(h_k)_{k \in \mathbb{N}}$ erfülle:
	
	$0 \le \frac{h_{k + 1}}{h_k} \le p < 1$ ($\Rightarrow h_k$ positiv monoton fallend)
	
	Dann gilt für $p_1^{(k)} \in P_n$ (in $h^q$) durch ($h_k^q, a(h_k)$), ..., ($h_{k +n}^q, a(h_{k + 1})$)
	
	$a(0) - p_n^{(k)}(0) = O(h_k^{(n + 1)q})$ \tab ($k \rightarrow \infty$)
	
	\newpage
	
	\section{Spline-Interpolation}
	
	\subsection{Interpolationsnachteil}
	
	Starke Oszillation von Polynomen höheren Grades
	
	\subsection{Abhilfe}
	
	Spline-Interpolation, d.h. stückweise polynomielle Interpolation mit (n - 1)-mal stetig diff.baren Knoten
	
	\subsection{Lineare Spline}
	
	alle Abschnitt-Splines sind lineare Funktionen
	
	\subsection{Kubischer Spline}
	
	$s_n: [a, b] \rightarrow \mathbb{R}$ kubischer Spline bezüglich $a = x_0 < x_1 < ... < x_n = b$, wenn gillt
	
	1. $s_n \in C^2[a, b]$
	
	2. $S_n|_{I_i} \in P_3$, i = 1, ..., n
	
	natürlicher Spline:
	
	3. $s_n''(a) = s_n''(b) = 0$
	
	\subsection{Existenz}
	
	Der interpolierende kubische Spline existiert und ist eindeutig bestimmt durch zusammen Vorgabe von $s_n''(a), s_n''(b)$
	
	für natürlichen Spline $s_n$ durch $x_0, ..., x_n$, $y_0, ..., y_n$ gilt:
	
	\tab $\int\limits_{a}^{b}|s'(x)|^2dx \le \int\limits_{a}^{b}|g''(x)|^2dx$
	
	bezüglich g $\in C^2[a, b]$ mit g($x_i$) = $y_i$, i = 1, ..., n
	
	\subsection{Approximationsfehler}
	
	$f \in C^4[a, b]$, $s_1''(a) = f''(a) \wedge s_n''(b) - f''(b)$:
	
	$\max\limits_{x \in [a, b]}|f(x) - s:n(x)| \le \frac{1}{2}h^4 \max\limits_{x \in [a, b]}|f^{(4)}(x)$
	
	\newpage
	
	\section{Gauß-Approximation}
	
	$<f, g>:= \int\limits_{a}^{b}f(t)\conj{g(t)}dt$ \tab $||f|| = \sqrt{<f, f>}$
	
	H Prähilbertraum, $\delta \subset H$ endlich Dimensional
	
	$\exists f \in H$ eindeutig bestimmte "beste Approximation" $g \in S$
	
	\tab $||f - g|| = \min\limits_{\varphi \in S}||f - \varphi||$
	
	bes. einfache Lösung, wenn \{$\varphi_1, ..., \varphi_n$\} eine ONB ist, d.h.
	
	$(\varphi_i, \varphi_j) = \delta_{i, j} \Rightarrow \alpha_i = <f, \varphi_i>$ \tab i = 1, ..., n
	
	\tab $\Rightarrow g = \sum\limits_{i = 1}^{n}<f, \varphi_i>\varphi_i$ ist beste Approximation
	
	\newpage
	
	\section{Gram-Schmidt-Algorithmus}
	
	$w_1 := \frac{v_1}{||v_1||}$ \space $\tilde{w_k}:=v_k - \sum\limits_{i = 1}^{k - 1}\gamma<v_k, w_i>w_i$, \space $w_k := \frac{\tilde{w_k}}{||\tilde{w_k}||}$
	
	\subsection{Code}
	
	n = size(v, 1)
	
	k = size(v, 2)
	
	u = np.zeros(n, k)
	
	u[:, 1] = v[:, 1]/sqrt(v[:, 1] * v[:, 1])
	
	for i in range(2, k):
	
	\tab u[:, i] = v[:, i]
	
	\tab for j in range(1, i - 1):
	
	\tab \tab u[:, i] = u[:, i] - (u[:, i] * u[:, j]) / (u[:, j] * u[:, j]) * u[:, j]
	
	\tab u[:, i] = u[:, i] / sqrt(u[:, i] * u[:, i])
	
	\newpage
	
	\section{Interpolatorische Quadraturformeln}
	
	$I(f) = \int\limits_{a}^{b}f(x)dx \approx I^{(n)}(f) = \sum\limits_{i = 1}^{n}\alpha_if(x_i)$
	
	Stützstellen a $\le a_0 < x_1 < ... < x_n \le b$ und Gewichte $\alpha_i \in \mathbb{R}$
	
	\subsection{Interpolatorische Quadratur Formel}
	
	$I^{(n)}(f) = \int\limits_{a}^{b}p_n(x)dx = \sum\limits_{i = 0}^nf(x_i)\int\limits_a^bL_i^{(n)}(x)dx$\footnote{$\alpha_i$}
	
	Lagrange:
	
	$I(f) - I^{(n)}(f) = \int\limits_a^bf[x_0, ..., x_n, x] \prod\limits_{i = 0}^n(x - x_i)dx$
	
	$\int\limits_a^bf(x)dx \approx \int\limits_a^bp_0(x)dx = (b - a) * \sum\limits_{i = 0}^nw_if(x_i)$
	
	$w_i = \frac{1}{(b - a)} \int\limits_a^bL_i(x)dx$
	
	\subsection{Ordnung}
	
	$I^{(n)}von der Ordnung m \Leftrightarrow \forall p \in P_{m - 1}$
	
	$\int\limits_a^bp(x)dx = I^{(n)}(p)$ \tab exakt
	
	$\Rightarrow$ Interpolatorische Quadraturformel zu (n + 1)-Stützstellen sind mindestens von der Ordnung n + 1
	
	$\Rightarrow$ höchstens Ordnung 2n + 2, mindestens n + 1
	
	\subsection{Newton-Cotes-Formel*}
	
	äquidistante Stützstellen
	
	\subsection{Abgeschlossene Formeln}
	
	$H = \frac{b - a}{n}, x_i = a + iH, a = x_0, b = x_n$
	
	Trapezregel: $I^{(1)}(f) = \frac{b - a}{2}[f(a) + f(b)]$
	
	Simpsonregel: $I^{(2)}(f) = \frac{b - a}{6}[f(a) + 4f(\frac{a + b}{2}) + f(b)]$
	
	$\rfrac{3}{8}-Regel: I^{(3)}(f) = \frac{b - a}{8}[f(a) + 3f(a + H) + 3f(b - h) + f(b)]$
	
	\subsection{Offene Formeln}
	
	($H = \frac{b - a}{n + 2}, x_i = a + (i + 1)H, a < x_0, x_n < b$)
	
	$I^{(0)}(f) = (b - a)f(\frac{a + b}{2})$ \tab Mittelpunktregel
	
	$I^{(1)}(f) = \frac{(b - a)}{2}(f(a + H) + f(b - H))$
	
	$I^{(2)}(f) = \frac{(b-  a)}{3}(2f(a + H) - f(\frac{a + b}{2}) + 2f(b - H))$
	
	\subsection{Code}
	
	\subsection{Problem}
	
	negative Gewichte $\alpha_i \Rightarrow$ Auslöschungsgefah
	
	Oszilationen des Lagrange Interpolanten (Runge-Phänomen) 
	
	$\Rightarrow I^{(n)}(f)\cancel{\xrightarrow{n \rightarrow\infty}}I(f) $
	
	\subsection{Abhilfe: Summierte Quadraturformeln *}
	
	$I-n^{(n)}(f) = \sum\limits_{i = 1}^{N - 1}I_{[x_i, x_i + 1]}^{(n)}(f)$ \tab $h = \frac{b - a}{N}, x_i = a + iH$
	
	\subsubsection{Fehlerdarstellung}
	
	$I_{[x_i, x_{i + 1}]}(f) - I_{[x_i, x_{i + 1}]}^{(n)}(f) = w_nh^{n + 2}f^{(m + 1)}(\xi_i)$, \tab $\xi_i \in [a, b]$
	
	$m \ge n: I(f) - I_n^{(n)}(f) = w_nh^{(m + 1)}(b - a)f^{(m + 1)}(\xi)$
	
	\newpage
	
	\section{Gaußsche Quadraturformeln *}
	
	\subsection{Gewichtetes Skalarprodukt}
	
	$<f, g>_\omega = \int\limits_a^bf(x)g(x)\omega(x)dx$, \tab $\omega(x)\ge 0, x \in (a, b)$
	
	\subsection{Gauß-Quadratur}
	
	$\exists!$ interpolierte Quadraturformel [?] (n + 1) paarweise verschiedene Stützstellen auf [-1, b] mit Ordnung 2n + 2. Stützstellen = Nullstellen
	
	$\alpha_i = \int\limits_{-1}^1\prod\limits_{j = 0, j \ne i}(\frac{x - \lambda_j}{\lambda_i - \lambda_j})^2 dx > 0$\footnote{Positivität der Gewichte}, \tab i = 0, ..., n
	
	$f\in C^{2n + 2}([-1, 1]) Restglied:$
		
	$R^{(n)} = \frac{f^{(2n + 2)}(\xi)}{(2n + 2)!}\int\limits_{-1}^1\prod\limits_{j = 0}^n(x - \lambda_j)^2dx$, \tab $\xi \in (-1 , 1)$
		
	\subsection{Wahl der Stützstellen}
	
	Nullstellen $\lambda_0, ..., \lambda_n \in(-1, 1)$ des (n + 1)-sten Legendre-Polynomes $L_{n + 1} \in P_{n + 1}$
	
	
	\subsection{Kongergenz der Gauß-Quadraturen}
	
	Sei $I^{(n)}(f)$ die (n + 1) punktige [?] Gauß-Formel zu $I(f) = \int\limits_{-1}^1f(x)dx \forall f \in C[-1, 1]: I^{(n)}(f) \xrightarrow{n \rightarrow \infty} I(f)$
	
	\subsection{Code}
	
	\newpage
	
	\section{Störungssattz...}
	
	\subsection{Störungssatz}
	
	$A \in \mathbb{K}^{n x n}$ regulär mit $||\delta A|| \le \frac{1}{||A^{-1}||}$, dann gilt für die
	
	\subsection{gestörte Matrix}
	
	$\tilde{A} = A + \delta A$ ist regulär
	
	Für den relativen Fehler der Lösung gilt mit Konditionszahl von A:
	
	\tab $cond(A) = ||A|| * ||A^{-1}||$
	
	die Ungleichung:
	
	\tab $\frac{||\delta_x||}{||x||} \le \frac{cond(A)}{1 - cond(A) \frac{||\delta A||}{||A||}}\left[ \frac{||\delta b||}{||b||} + \frac{||\delta A||}{||A||}\right] $
	
	\newpage
	
	\section{Lösung von Dreieckssystemen+Aufwand*}
	
	\subsection{Rückwärtseinsetzen}
	
	\[
		x_j=\left\{
			\begin{array}{ll}
				\frac{b_n}{a_{nn}} & j = n\\
				\frac{1}{a_{jj}}(b_j - \sum\limits_{k= j + 1}^na_{jk}x_k) & j = n - 1, ..., 1
			\end{array}
		\right.
	\]
	
	\subsection{Vorwärtseinsetzen}
	
	\[
		x_j=\left\{
			\begin{array}{ll}
				\frac{b_n}{a_{nn}} & j = n\\
				\frac{1}{a_{jj}}(b_j - \sum\limits_{k= j + 1}^na_{jk}x_k) & j = 1, ..., n - 1
			\end{array}
		\right.
	\]
	
	\subsection{Aufwend}
	
	$\sum\limits_{j = 1}^nj = \frac{(n + 1)n}{2} = \frac{n^2}{2} + O(n)$
	
	\newpage
	
	\section{Gaußsches Eliminationsverfahren...}
	
	\subsection{Gauß-Elimination}
	
	Umformen von Ax = b auf Rx = c, R ist eine rechte obere Dreieck Matrix
	
	\subsection{Spaltenpivotisierung}
	
	$|a_{r_{k, k}}^{(k - 1)}| = \max\limits_{j = k, ..., n}|a_{jk}^{(k - 1)}|$
	
	\subsection{LR - Zerlegung}
	
	Ax = b
	
	$\begin{pmatrix}
		2 & 1 & 7 \\
		8 & 8 & 33 \\
		-4 & 10 & 4
	\end{pmatrix}$
	x =
	$\begin{pmatrix}
		15 \\
		73 \\
		12
	\end{pmatrix}$
	$\bordermatrix{ ~ & & & \cr
		 & 8 & 8 & 33 \cr
		(-4) * & 2 & 1 & 7 \cr
		2 * & -4 & 10 & 4
	}$
	$P_1 = \begin{pmatrix}
		0 & 1 & 0 \\
		1 & 0 & 0 \\
		0 & 0 & 1
	\end{pmatrix}$
	
	Eliminierung
	
	$\begin{pmatrix}
		8 & 8 & 33 \\
		2 & 1 & 7 \\
		-4 & 10 & 4
	\end{pmatrix}
	L_1 = \begin{pmatrix}
		1 & 0 & 0 \\
		\frac{1}{4} & 1 & 0 \\
		-\frac{1}{2} & 0 & 1
	\end{pmatrix}
	\rightsquigarrow
	\begin{pmatrix}
		8 & 8 & 33 \\
		0 & 4 & 5 \\
		0 & 28 & 41
	\end{pmatrix}
	P_2 = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & 0 & 1 \\
		0 & 1 & 0
	\end{pmatrix}$
	
	$\bordermatrix{ ~ & & & \cr
		& 8 & 8 & 33 \cr
		& 0 & 28 & 41 \cr
		(-7) * & 0 & 4 & 5
	}
	L_2 = \begin{pmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & \frac{1}{7} & 1
	\end{pmatrix}
	\rightsquigarrow
	\begin{pmatrix}
		8 & 8 & 33 \\
		0 & 28 & 41 \\
		0 & 0 & 6
	\end{pmatrix}
	 = R$
	
	$PA = LR \Rightarrow L_2L_1 P_2P_1 A = F \Rightarrow P_2P_1A = L_1^{-1}L_2^{-1}R, P_2P_1 = P$
	
	$\begin{pmatrix}
		1 & 0 & 0 \\
		0 & 0 & 1 \\
		0 & 1 & 0
	\end{pmatrix}
	\begin{pmatrix}
		0 & 1 & 0 \\
		1 & 0 & 0 \\
		0 & 0 & 1
	\end{pmatrix}
	=
	\begin{pmatrix}
		0 & 0 & 1 \\
		1 & 0 & 0 \\
		0 & 1 & 0
	\end{pmatrix}$
	
	$L_2L_1 = (\ddots), (L_2L_1)^{-1} = L = ([linke untere dreiecks matrix])$
	
	$Ax = b \Rightarrow LRx = Pb, Pb = \tilde{b}$
	
	$\Rightarrow Ly = \tilde{b}, Rx = y$
	
	A regulär + diagonaldominant $\Rightarrow$ A = LR kann ohne Pivot bezeichnet werden
	
	\subsection{Code}
	
	\newpage
	
	\section{Symmetrisch positiv definite Systeme}
	
	\subsection{Cholesky-Zerlegung}
	
	Jede symmetrische positiv definite Matrix A hat eine sogenannte Cholesky - Zerlegung:
	
	$A = LDL^t = \tilde{L}\tilde{L}^t$, $\tilde{L} := LD^{\frac{1}{2}}$
	
	$\begin{pmatrix}
		\tilde{l}_{1,1} & \dots & 0 \\
		\vdots & \ddots & \vdots \\
		\tilde{l}_{n,1} & \dots & \tilde{l}_{n,n}
	\end{pmatrix}$
	$\begin{pmatrix}
		\tilde{l}_{1,1} & \dots & \tilde{l}_{n,1} \\
		\vdots & \ddots & \vdots \\
		0 & \dots & \tilde{l}_{n,n}
	\end{pmatrix}$
	=
	$\begin{pmatrix}
		a_{1,1} & \dots & a_{1, n} \\
		\vdots & \ddots & \vdots \\
		a_{n, 1} & \dots & a_{n, n}
	\end{pmatrix}$
	
	$i \ge j: a_{i, j} = \sum\limits_{k = 1}^j\tilde{l}_{i, k}\tilde{l}_{j, k} = \sum\limits_{k = 1}^{j - 1}\tilde{l}_{i, k}\tilde{j, k} + \tilde{l}_{i, j}\tilde{l}_{j, j}$
	
	i = 1, ..., n \tab $\tilde{l}_{i, i} = \sqrt{a_{i, i} - \sum\limits_{k = 1}^{i - 1}\tilde{l}_{i, k}^2}$
	
	j = i + 1, ..., n \tab $ \tilde{l}_{i, j} = \frac{1}{\tilde{l}_{i, i}}(a_{i, j} - \sum\limits_{k = 1}^{i - 1}\tilde{l}_{i, k}\tilde{l}_{j, k})$
	
	Dandmatrizen: Nullen nicht speichern/berechnen Diagonal-Dominante: keine Pivotisierung notwendig symmetrisch positiv definite: keine Pivotisierung notwending
	
	\subsection{Aufwand}
	
	$N_{Cholesky}(n) = \frac{n^3}{6} + O(n^2)$ \tab (billiger als A = LR)
	
	\subsection{Code}
	
	\newpage
	
	\section{Least-Squares-Lösungen, Normalgleichung}
	
	$A \in \mathbb{R}^{m \times n} Ax = b$
	
	keine Lösung, $b \notin im(A)$
	
	unendlich viele Lösungen $\bar{x} + \delta x \Leftrightarrow A\bar{x}= b$, $\delta x \in ker(A) \ne \{0\}$
	
	\subsection{Least-Squares-Lösung}
	
	Es existiert immer eine "Lösung" $\bar{x} \in \mathbb{R}^n$ mit kleinsten Fehlerquadraten
	
	\subsection{Eindeutigkeit}
	
	R(A) = n $\Leftrightarrow \bar{x}$ eindeutig, jede weitere Lösung: $\bar{x} + y, y \in ker(A)$
	
	Gerade: b = C + Dt 
	$\begin{pmatrix}
		x_1 \\
		y_1
	\end{pmatrix}
	\begin{pmatrix}
	x_2 \\
	y_2
	\end{pmatrix}
	\begin{pmatrix}
	x_3 \\
	y_3
	\end{pmatrix}$
	
	$\begin{matrix}
		\Rightarrow & C + x_1D = y_1 \\
		& C + x_2D = y_2 \\
		& C + x_3D = y_3 
	\end{matrix}$
	
	$\Rightarrow A = \begin{bmatrix}
		1 & x_1 \\
		1 & x_2 \\
		1 & x_3 
	\end{bmatrix}
	x = 
	\begin{bmatrix}
		C \\
		D
	\end{bmatrix}
	b = 
	\begin{bmatrix}
		y_1 \\
		y_2 \\
		y_3
	\end{bmatrix}$
	
	1. C = $A^tA$ = [ ], b' = $A^tb$ = [ ]
	
	2. Cholesky Zerlegung: $G^tG$ = C $\Rightarrow$ G = [ ]
	
	3. $G^ty = b' \Rightarrow y = [ ], Gx = y \Rightarrow x = \begin{bmatrix}
		a_1 \\
		a_2
	\end{bmatrix}$
	
	$\Rightarrow Gerade: b = a_1 + a_2t$
	
	Alternativ: $Q^tA = R = \begin{bmatrix}
		a_1 \\
		a_2
	\end{bmatrix}
	, Gx = y \Rightarrow x = \begin{bmatrix}
	a_1 \\
	a_2
	\end{bmatrix}$
	
	$R_1x = \tilde{b}_1 \Rightarrow D = a_1, C = A_2$
	
	\newpage
	
	\section{QR-Zerlegung, ...}
	
	\subsection{QR-Zerlegung}
	
	$A \in \mathbb{K}^{m \times n}, Rang(A) = n \le m$ 
	
	$\exists \text{eindeutig bestimmte Matrix Q} \in \mathbb{K}^{m \times n}$
	\begin{tabular}{l l}
		& $Q^tQ = En (\text{für }\mathbb{K} = \mathbb{R})$  \\
		& $\conj{Q}^t Q = En (\text{für }\mathbb{K} = \mathbb{C})$
	\end{tabular}
	
	und eindeutig bestimmte obere Dreiecks Matrix $R \in \mathbb{K}^{n \times n} r_{i, i} < 0$ reell, s.d.
	
	A = QR \tab Q orthonormale Matrix (m = n unitär) $Q^t = Q^{-1}$
	
	u := v + $\mathfrak{S} ||v|| * e_n$
	
	$\mathfrak{S}$
	\[
		\left \{
			\begin{array}{l l}
				-1 & v_1 < 0 \\
				1 & v_1 \ge 0
			\end{array}
		\right. \text{Householder Matrix }H = E_n - 2 \frac{uu^t}{u^tu}
	\]
	
	\subsection{Least-Squares mit Vollrang}
	
	\begin{tabular}{l l l }
		$A = Q_1R = (Q_1|Q_2)$ &
		$\begin{pmatrix}
			R \\
			0
		\end{pmatrix}$ &
		, $Q = (Q_1|Q_2) \in \mathbb{R}^{m \times n}$ \\
		$Rx = Q^tb$ & & R = 
		$\begin{pmatrix}
			R \\
			0
		\end{pmatrix}$
		$\in \mathbb{R}^{m \times n}$ \\
		& & Rang(A) = n
	\end{tabular}
	
	$||Ax - b||_2^2 = ||Rx - Q_1^tb||_2^2 + ||Q_2^tb||_2^2 \text{ minimal für } x = R^{-1}Q_1^tb$
	
	\subsection{Aufwand}
	
	Doppelter Aufwand für QR wie für LR
	
	$N_{QR}(n) = \frac{2}{3}n^3 + O(n^2)$
	
	\subsection{Code}
	
	\newpage
	
	\section{Householder-Transformation, ...}
	
	\subsection{Householder Transformation}
	
	$v \in \mathbb{K}$, $||v||_2 = 1$
	
	$H = E_n - 2 \frac{vv^t}{v^tv}$
	
	Spiegelung eines Vektors an einer Hyperebene durch Null im euklidischen Raum $H_{\overrightarrow{x}} = x - 2vv^tx$
	
	\begin{equation*}
		v\conj{v}^t =
		\begin{pmatrix}
			v_1\conj{v}_1 & \dotsc & v_1\conj{v}_n \\
			\vdots & \ddots & \vdots \\
			v_n\conj{v}_1 & \dotsc & v_n\conj{v}_n
		\end{pmatrix}
	\end{equation*}
	
	TODO: skizze
	
	\subsection{Eigenschaften von Householder}
	
	Symmetrisch: $H = H^t$
	
	Orthogonal: $H^tH = E_n$
	
	Involutorisch: $h^2 = E_n$
	
	EW: -1 einfach, 1 (n - 1)-fach
	
	Mantisse-Vektor-Multiplikationen mit Householder sind schnell berechenbar
	
	\subsection{Householder-Verfahren}
	
	Gruppe numerischer Verfahren zur Bestimmung von Nullstellen einer skalaren, reellen Funktion
	
	Zur Berechnung der QR-Zerlegung
	
	$S_tA^{(t - 1)} = A^{(t)}$, $S_t$ ist Householder-Transformation
	
	\subsection{Ergebnis}
	
	QR-Zerlegung von A (aber \underline{nicht}eindeutig!)
	
	\newpage
	
	\section{Intervallschachtelung/Bisektionsverfahren}
	
	\subsection{Intervallschachtelung/Bisektionsverfahren}
	
	n = 1
	
	Erzeugen einen konvergenten Folge von Intervallschachtelungen
	
	$a_0, b_0 \in [a, b]$ mit $f(a_0)f(b_0) < 0$
	
	Konvergenz: $a_k \le a_{k + 1} \le b_{k + 1} \le b_k$
	
	$b_{k + 1} - a_{k + 1}| = \frac{1}{2}|b_k - a_k| = 2^{-k - 1}|b_0 - a_0|$
	
	\subsection{Eigenschaften}
	
	sehr stabil, langsam, Erweiterung für $x \in \mathbb{R}^n \vee x \in \mathbb{C}$ $\underline{nicht}$ möglich
	
	\subsection{Code}
	
	for k = 0, 1, ... do:
	
	\tab x[k] = 0.5(a[k] + b[k])
	
	\tab if f(a[k])f(x[k]) < 0:
	
	\tab \tab a[k + 1] = a[k]
	
	\tab \tab b[k + 1] = x[k]
	
	\tab else:
	
	\tab \tab a[k + 1] = x[k]
	
	\tab \tab b[k + 1] = b[k]
	
	\tab if abs(b[k + 1] - a[k + 1]) < TOL abs(a[k + 1]):
	
	\tab \tab Ende Lösung: 0.5(b[k + 1] + a[k + 1])
	
	\newpage
	
	\section{Konvergenz interativer Methoden}
	
	\subsection{Konvergenzordnung}
	
	$\alpha > 1$
	
	$|x_{k + 1} - x_*| \le C|x_k - x_*|^\alpha$ \tab k = 0, 1, ...
	
	\subsection{Lineare Konvergenz}
	
	$\alpha = 1 \Rightarrow C lineare Kontraktionsrate$
	
	\subsection{Superlineare Konvergenz}
	
	$|x_{k + 1} - x_k| \le C_k |x_k - x_*|$ \tab mit $c_k \rightarrow 0$
	
	\subsection{Quadratische Konvergenz}
	
	$|x_{k + 1} - x_*| \le C |x_k - x_*|$ \tab also $\alpha = 2$
	
	Konvergenz der Ordnung $\alpha$ (quadratische Konvergenz, kubische, etc.) $\Rightarrow$ superlineare Konvergenz $\Rightarrow$ lineare Konvergenz
	
	\newpage
	
	\section{Newton Verfahren im \texorpdfstring{$\mathbb{R}^n$}{R pow(n)}}
	
	$x_{k + 1} = x_k - j_f(x_k)^{-1}f(x_k)$
	
	$j_f(x_k)^{-1} = \frac{1}{det(j_f)} * j_f^*$, 
	$j_f^* = 
	\begin{pmatrix}
		a_{2, 2} & -a_{1, 2} \\
		-a_{2, 1} & a_{1, 1}
	\end{pmatrix}$
	
	\begin{tabular}{l l}
		guter Startwert: & superlineare Konvergenz \\		
		schlechter Startwert\footnote{Abstand Startwert zur Nullstelle}: & lineare Konvergenz
	\end{tabular}
	
	\subsection{Code}
	
	\newpage
	
	\section{Newton-Kantorisch}
	
	\subsection{Voraussetzungen}
	
	$f: x \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ mit f' L-stetig, d.h.
	
	1. $|| j_f(x) - j_f(y)|| \le L||x - y||$
	
	2. $||j_f^{-1}(x)|| \le \beta$
	
	3. $x_0 \in D_f(x)$
	
	4. $q := \frac{1}{2} \alpha \beta L$ mit $\alpha = ||j_f^{-1}(x_0)f(x_0)||$ auf der Nieveaumenge $D(x) = {y \in D | ||f(x) || \le ||x||}$
	
	$\Rightarrow$ Dann konvergiert $(x_k)$ quadratisch gegen Nullstelle $z \in D$ von f
	
	\subsection{Fehlerabschätzung}
	
	$||x_k - x_*|| \le \frac{\alpha}{1 - q}q^{(2^k - 1)}$ \tab , $k \ge 1$
	
	\newpage
	
	\section{Sukzessive Approximation *}
	
	$x_{t + 1} = x_t + C^{-1} f(x_t)$ \tab C reguläre Matrix $\in \mathbb{R}^{n \times n}$
	
	$x_t \xrightarrow[t \rightarrow \infty] z \Rightarrow z = z + C^{-1}f(z)$ bzw $f(z) = 0 \Rightarrow$ z Fixpunkt der Abbildung $g(x) := x + C^{-1}f(x)$
	
	\subsection{Konvergenz}
	
	$g: G \rightarrow G[?]$ Kontraktion, $\exists$ Fixpunkt $z \in G von g x_t \rightarrow z$, q Kontraktionskonstante
	
	$||x_t - z|| \le \frac{q}{1 - q} ||x_t - x_{t - 1} \le \frac{q_t}{1 - q} ||x_1 - x_0||$
	
	\subsection{Code}
	
	\newpage
	
	\section{Newton-Verfahren für affin - lineares f(x) = Ax - b}
	
	\subsection{Problem}
	
	Direkte Methoden $\rightarrow$ großen Speicheraufwand für große n
	
	\newpage
	
	\section{Fixpunktiteration *, Konvergenzaussage}
	
	\subsection{Problem}
	
	für sehr große n ist Gauß-Elimination zu speicherintensiv
	
	$Ax = b \Leftrightarrow a_{j, j} * x_j + \sum\limits_{k = 1}^na_{j, k}x_k = b_j$
	
	\subsection{Aufspaltung}
	
	A = D + L + R
	
	D = 
	$\begin{pmatrix}
		a_{1, 1} & & 0 \\
		& \ddots & \\
		0 & & a_{n, n}
	\end{pmatrix}$
	L = 
	$\begin{pmatrix}
		0 & \dotsc & \dotsc & 0 \\
		a_{2, 1} & \ddots & \dotsc & \vdots \\
		\vdots & \ddots & \dotsc & \vdots \\
		a_{n, 1} & \dotsc & a_{n, n - 1} & 0
	\end{pmatrix}$
	R = 
	$\begin{pmatrix}
		0 & a_{1, 2} & \dotsc & a_{1, n} \\
		\vdots & \ddots & \dotsc & \vdots \\
		\vdots & \ddots & \dotsc & a_{n - 1, n} \\
		0 & \dotsc & \dotsc & 0
	\end{pmatrix}$
	
	$x_t = Bx_{t - 1} + c$, B Iterationsmatrix
	
	\subsection{Konvergenz}
	
	$x_t \xrightarrow[t \rightarrow \infty] x \Rightarrow x = Bx + c$
	
	$x_t \rightarrow x \Leftrightarrow spr(B) = max{|\lambda| | \lambda EW von B} < 1$
	
	Asymptotisches Konvergenzverhalten:
	
	$\sup\limits_{x_0 \in \mathbb{R}^n} \limsup\limits_{t \rightarrow \infty}(\frac{||x_t - x||}{x_0 - x})^{\frac{1}{t}} = spr(B)$
	
	\subsection{Code}
	
	\newpage
	
	\section{Jacobi *, Gauß-Seidel *, SOR *}
	
	\subsection{Jacobi-Verfahren}
	
	$B = J = -D^{-1}(L + R)$ mit A = D + L + R
	
	\subsection{Gauß-Seidel}
	
	$B = H_1 = -(D + L)^{-1}R$
	
	\subsection{SOR}
	
	$H_w = (D + wL)^{-1}((1 - w)D - wR)$
	
	(für w = 1: Gauß-Seidel)
	
	\subsection{Konvergenz}
	
	für positiv definiti symetrische Matrizen:
	
	(I) starke/strikte Diagonaldominanz $\Rightarrow$ J/G-S konvergiert
	
	(II) diagonaldominanz + ireduzibel\footnote{Knotenmodell} $\Rightarrow$ J/G-S konvergiert
	
	\begin{tabular}{l l l}
		(III) Beachte EW von: & $J = -D^{-1}(L + R)$ & J konvergiert für $spr(J) < 1$ \\
		& $H = -(D + L)^{-1}R$ & G-S konvergiert für $spr(H) < 1$
	\end{tabular}
	
	\subsection{Fehler verringern}
	
	Wie viele Iterationsschritte t bis Fehler um $10^k$ verbessert ist?
	
	$t \ge -\frac{k}{\log_{10}f}$ \tab f = spr(B)
	
	\newpage
	
	\section{Allgemeines Abstiegsverfahren}
	
	\subsection{Voraussetzung}
	
	A symmetrisch positiv definiert
	
	$\Rightarrow <Ax, y> = <x, Ay>, <Ax, x> > 0$
	
	\subsection{A-Skalarprodukt, A-Norm}
	
	$<x, y>_A = <x, Ay>$, $||x||_A = \sqrt{<Ax, y>}$
	
	A hat nur reelle EW
	
	$\lambda_{min} := \lambda_1 \le ... \le \lambda_n =: \lambda_{max} \Rightarrow spr(A) = \lambda_{max}, cond_2(A) \frac{\lambda_{max}}{\lambda_{min}}$
	
	\subsection{Abstiegsverfahren}
	
	Iteratives Verfahren, um lokales Minimum einer Funktion zu finden.
	
	In jedem Schritt entlang einer bestimmten Richtung minimieren
	
	\subsection{Gradient}
	
	$g_t = Ax_t - b$, Abstiegsrichtung $r_t$
	
	\subsection{Schrittweite}
	
	$\alpha_t = -\frac{<g_t, r_t>}{<Ar_t, r_t>}$
	
	\subsection{Iteration}
	
	$x_{t + 1} = x_t + \alpha_lr_t$
	
	$\Rightarrow$ Minimierung von Q minimiert 
	
	Defektnorm $||A_y - b||_{A^{-1}}$ und Fehlernorm $||y - x||_A$
	
	\subsection{Code}
	
	\newpage
	
	\section{Gradientenverfahren}
	
	Richtung des steilsten Abstiegs
	
	$r_t$ = -grad $Q(x_t) = -g_t$
	
	\subsection{Iteration}
	
	$x_0 \in \mathbb{R}^n, g_0 = Ax_0 - b, t \ge 0$
	
	$\alpha_t = \frac{||g_t||^2}{<Ag_t, g_t>}$
	
	$x_{t + 1} = x_t - \alpha_t g_t$
	
	$g_{t + 1} = g_t - \alpha_t Ag_t$
	
	$<Ag_t, g_t> = 0 \Rightarrow g_t = 0 \Rightarrow Ax_t = b$
	
	\subsection{Konvergenz}
	
	A positiv definit symmetrisch $\Rightarrow$ Gradientenverfahren konvergiert $\forall x_0$ gegen Ax = b
	
	\subsection{Lemma von Kantorich}
	
	$ 4 \frac{\lambda_{min} \lambda_{max}}{(\lambda_{min} + \lambda_{max})^2} \le \frac{||<^4||}{<y, Ay><y, A^-1y>}$
	
	\subsection{Fehlerabschätzung}
	
	$||x_t - x||_A \le (\frac{1 - K^{-1}}{1 + K^{-1}})^t ||x_0 - x||_a$ \tab , $t \in \mathbb{N}$
	
	$\Rightarrow$ Lagransche Konvergenz für $cond_2(A) >> 1$
	
	\newpage
	
	\section{CG-Verfahren}
	
\end{document}